{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The **Sequential model** makes the assumption that the network has exactly one input and exactly one output, and that it consists of a linear stack of layers.  \n",
    "\n",
    "But this set of assumptions is too inflexible in a number of cases. Some networks require **several independent inputs** (**Multi-input Models**), others require **multiple outputs**, and some networks have internal branching between layers that makes them look like **graphs of layers** rather than linear stacks of layers.\n",
    "\n",
    "## Introduction to the functional API\n",
    "\n",
    "In the functional API, you directly manipulate tensors, and you use layers as functions that take tensors and return tensors (hence, the name functional API):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Input, layers\n",
    "\n",
    "input_tensor = Input(shape=(32, ))              # a tensor\n",
    "\n",
    "dense = layers.Dense(32, activation='relu')     # a layer is a function\n",
    "\n",
    "output_tensor = dense(input_tensor)             # a layer may be called on a tensor, and it returns a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential vs Functional API\n",
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "# Sequential\n",
    "seq_model = Sequential()\n",
    "seq_model.add(layers.Dense(32, activation='relu', input_shape=(64, )))\n",
    "seq_model.add(layers.Dense(32, activation='relu'))\n",
    "seq_model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Functional equivalent\n",
    "input_tensor = Input(shape=(64,))\n",
    "x = layers.Dense(32, activation='relu')(input_tensor)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(input_tensor, output_tensor)          # the Model class turns an input tensor and output tensor into a model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only part that may seem a bit magical at this point is instantiating a Model object using only an input tensor and an output tensor. Behind the scenes, Keras retrieves every layer involved in going from input_tensor to output_tensor, bringing them together into a graph-like data structure—a Model. Of course, the reason it works is that output_tensor was obtained by repeatedly transforming input_tensor. If you tried to build a model from inputs and outputs that weren’t related, you’d get a RuntimeError:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"input_2:0\", shape=(?, 64), dtype=float32) at layer \"input_2\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ad090d276832>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0munrelated_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbad_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munrelated_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/araks/anaconda/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/araks/anaconda/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m   1807\u001b[0m                                 \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m                                 \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1809\u001b[0;31m                                 str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1810\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m                         \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"input_2:0\", shape=(?, 64), dtype=float32) at layer \"input_2\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "unrelated_input = Input(shape=(32,))\n",
    "bad_model = model = Model(unrelated_input, output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the compiling, evaluating such an instance of Model is the same\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 1s 899us/step - loss: 11.6534\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 11.5413\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 11.5271\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.5182\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 11.5128\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 11.5089\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 11.5068\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 11.5049\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.5028\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 11.5007\n",
      "1000/1000 [==============================] - 0s 82us/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_train = np.random.random((1000, 64))\n",
    "y_train = np.random.random((1000, 10))\n",
    "\n",
    "# train\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128)\n",
    "\n",
    "# evaluate\n",
    "score = model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-input Models\n",
    "\n",
    "A question-answering model. Such models usually have two inputs: a natural-language question and a test snippet (such as a news article) providing information to be used for answerining the question.  \n",
    "\n",
    "The model must then produce an answer: in the simplest possible setup, this is a one-word answer obtained via a softmax over some predefined vocabulary.\n",
    "\n",
    "Following is an example of how you can build such a model with the functional API. You set up two independent branches, encoding the text input and the question input as representation vectors; then, concatenate these vectors; and finally, add a softmax classifier on top of the concatenated representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Here is a valuable note for anyone reading this book and trying to run the exercised. In the embedding layers below, the book gives the following lines:\n",
    "                                             \n",
    "                                              ...\n",
    "                embedded_text = layers.Embedding(64, text_vocabulary_size)(text_input)\n",
    "                                              ...\n",
    "                embedded_question = layers.Embedding(32, question_vocabulary_size)(question_input)\n",
    "                \n",
    "While the correct is the following:\n",
    "                            \n",
    "                                              ...\n",
    "                embedded_text = layers.Embedding(text_vocabulary_size)(text_input, 64)\n",
    "                                              ...\n",
    "                embedded_question = layers.Embedding(question_vocabulary_size, 32)(question_input)\n",
    "                \n",
    "**Where I found the solution**: I found how to fix this with the help of the following page:  https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/ , which says:\n",
    "\n",
    "The embedding must specify 3 arguments:\n",
    "\n",
    "* **input_dim**: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "* **output_dim**: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "* **input_length**: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n",
    "\n",
    "So in the book the places of **input_dim** and **output_dim** are switched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "text_vocabulary_size = 10000\n",
    "question_vocabulary_size = 10000\n",
    "answer_vocabulary_size = 500\n",
    "\n",
    "text_input = Input(shape=(None,),                                            \n",
    "                   dtype='int32', \n",
    "                   name='text')                                        # the text input is a variable-length sequence of integers. Note that you can optionally name the inputs.\n",
    "embedded_text = layers.Embedding(text_vocabulary_size, 64)(text_input) # embeds the inputs into a sequence of vectors of size 64\n",
    "encoded_text = layers.LSTM(32)(embedded_text)                          # encodes the vectors in a single vector via an LSTM\n",
    "\n",
    "question_input = Input(shape=(None,), \n",
    "                       dtype='int32',\n",
    "                      name='question')                                 # Same process (with different layer instance) for the question\n",
    "embedded_question = layers.Embedding(question_vocabulary_size, 32)(question_input)\n",
    "encoded_question = layers.LSTM(16)(embedded_question)\n",
    "\n",
    "concatenated = layers.concatenate([encoded_text, encoded_question], \n",
    "                                 axis=-1)                              # concatenates the encoded question and encoded text\n",
    "answer = layers.Dense(answer_vocabulary_size,\n",
    "                     activation='softmax')(concatenated)                # Adds a softmax classifier on top\n",
    "\n",
    "model = Model([text_input, question_input], answer)       # At model instantiation, you specify the two inputs and the output\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two possible APIs: feed the model a list of Numpy arrays as inputs, or feed it a dictionary that maps input names to Numpy arrays (if you give names to your inputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feeding data to a multi-input model\n",
    "import numpy as np\n",
    "\n",
    "num_samples = 1000\n",
    "max_length = 100\n",
    "\n",
    "text = np.random.randint(1, text_vocabulary_size,\n",
    "                        size=(num_samples, max_length))                   # Generates dummy Numpy data\n",
    "question = np.random.randint(1, question_vocabulary_size,\n",
    "                            size=(num_samples, max_length))\n",
    "answers = np.random.randint(0, 1, \n",
    "                           size=(num_samples, answer_vocabulary_size))    # answers are one-hot encoded, not integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12df1f390>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting using a list of inputs\n",
    "model.fit([text, question], answers, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1335d2da0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting using a dictionary of inputs (only if inputs are named)\n",
    "model.fit({'text': text, 'question': question}, answers,\n",
    "         epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Multi-output models\n",
    "\n",
    "In the same way, you can use the functional API to build models with multiple outputs (or multiple **heads**). A simple example is a network that attempts to simultaneously predict different properties of the data, such as a network that takes as input a series of social media posts from a single anonymous person and tries to predict attributes of that person, such as age, gender, and income level.\n",
    "\n",
    "**Note:** Below is the same error in Embedding step. We discussed it above. Here are the wrong and right ways:\n",
    "\n",
    "             embedded_posts = layers.Embedding(256, vocabulary_size)(posts_input)   # wrong\n",
    "             embedded_posts = layers.Embedding(vocabulary_size, 256)(posts_input)   # right\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: I commented the second set of convolutional layers because with that layers the computed output would be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functional API implementation of a three-output model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "\n",
    "vocabulary_size = 50000\n",
    "num_income_groups = 10\n",
    "\n",
    "posts_input = Input(shape=(None,), dtype='int32', name='posts')\n",
    "embedded_posts = layers.Embedding(vocabulary_size, 256)(posts_input)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(embedded_posts)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "#x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "#x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "#x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "age_prediction = layers.Dense(1, name='age')(x)                      # Note that output layers are given names\n",
    "income_prediction = layers.Dense(num_income_groups,\n",
    "                                activation='softmax',\n",
    "                                name='income')(x)\n",
    "gender_prediction = layers.Dense(1, activation='sigmoid', name='gender')(x)\n",
    "\n",
    "model = Model(posts_input,\n",
    "             [age_prediction, income_prediction, gender_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compilation options of a multi-output model: multiple losses\n",
    "## the different loss values are summed into a global loss, which is minimized during training.\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'])\n",
    "\n",
    "# equivalent (possible only of you give names to the output layers)\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss={'age': 'mse',\n",
    "                  'income': 'categorical_crossentropy',\n",
    "                  'gender': 'binary_crossentropy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that very **imbalanced loss contributions will cause the model representations to be optimized preferentially for the task with the largest individual loss**, at the expense of the other tasks. To remedy this, you can assign different levels of importance to the loss values in their contribution to the final loss. This is useful in particular if the losses’ values use different scales. For instance, the mean squared error (MSE) loss used for the age-regression task typically takes a value around 3–5, whereas the cross- entropy loss used for the gender-classification task can be as low as 0.1. In such a situa- tion, to balance the contribution of the different losses, you can assign a weight of 10 to the crossentropy loss and a weight of 0.25 to the MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compiling options of a multi-output model: loss weighting\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'],\n",
    "             loss_weights=[0.25, 1., 10.])\n",
    "\n",
    "# Equivalent (possible only if you giva nems to the output layers)\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss={'age': 'mse',\n",
    "                  'income': 'categorical_crossentropy',\n",
    "                  'gender': 'binary_crossentropy'},\n",
    "             loss_weights={'age': 0.25,\n",
    "                          'income': 1.,\n",
    "                          'gender': 10.})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** An example data is not provided by the book so I generated some dummy data to run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples = 1000\n",
    "max_length = 100\n",
    "\n",
    "posts = np.random.randint(1, vocabulary_size,\n",
    "                        size=(num_samples, max_length))                   # Generates dummy Numpy data\n",
    "\n",
    "age_targets = np.random.randint(1, 100,\n",
    "                       size=(num_samples, 1))\n",
    "\n",
    "income_targets = np.random.randint(0, 1,\n",
    "                          size=(num_samples, num_income_groups))\n",
    "\n",
    "gender_targets = np.random.randint(0, 1,\n",
    "                          size=(num_samples, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 19s 19ms/step - loss: 318.1101 - age_loss: 1268.5261 - income_loss: 0.0000e+00 - gender_loss: 0.0979\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 124.2715 - age_loss: 497.0490 - income_loss: 0.0000e+00 - gender_loss: 9.2373e-04\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 46.5534 - age_loss: 186.1135 - income_loss: 0.0000e+00 - gender_loss: 0.0025\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 46.6748 - age_loss: 186.5206 - income_loss: 0.0000e+00 - gender_loss: 0.0045\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 36.7082 - age_loss: 146.7206 - income_loss: 0.0000e+00 - gender_loss: 0.0028\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: 34.7394 - age_loss: 138.8525 - income_loss: 0.0000e+00 - gender_loss: 0.0026\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 37.2478 - age_loss: 148.9139 - income_loss: 0.0000e+00 - gender_loss: 0.0019\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 33.2299 - age_loss: 132.8543 - income_loss: 0.0000e+00 - gender_loss: 0.0016\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 28.6590 - age_loss: 114.5797 - income_loss: 0.0000e+00 - gender_loss: 0.0014\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 30.3334 - age_loss: 121.2777 - income_loss: 0.0000e+00 - gender_loss: 0.0014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13cc9b908>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feeding data to a multi-output model (Numpy data or a dictionary of arrays)\n",
    "model.fit(posts, [age_targets, income_targets, gender_targets],\n",
    "         epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
