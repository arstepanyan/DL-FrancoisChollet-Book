{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blueprint is: **problem definition**, **evaluation**, **feature engineering**, and **fighting overfitting**.\n",
    "\n",
    "1. **Defining the problem and assembling a dataset**\n",
    "2. **Choosing a measure of success**.   \n",
    "    Some useful notes:\n",
    "    * For balanced-classification problems, where every class is equally likely, accuracy and area under the receiver operating characteristic curve (ROC AUC) are common metrics. For class-imbalanced problems, you can use precision and recall. For ranking problems or multilabel classification, you can use mean average precision. \n",
    "3. **Deciding on an evaluation protocol**  \n",
    "    Three common evaluation protocols:\n",
    "      * Maintaining a hold-out validation set—The way to go when you have plenty of data\n",
    "      * Doing K-fold cross-validation—The right choice when you have too few samples for hold-out validation to be reliable\n",
    "      * Doing iterated K-fold validation—For performing highly accurate model evalua- tion when little data is available\n",
    "4. **Preparing your data**  \n",
    "    Once you know what you’re training on, what you’re optimizing for, and how to evalu- ate your approach, you’re almost ready to begin training models. But first, you should format your data in a way that can be fed into a machine-learning model—here, we’ll assume a deep neural network:\n",
    "    * As you saw previously, your data should be formatted as tensors.\n",
    "    * The values taken by these tensors should usually be scaled to small values: for example, in the [-1, 1] range or [0, 1] range. \n",
    "    * If different features take values in different ranges (heterogeneous data), then the data should be normalized.\n",
    "    * You may want to do some feature engineering, especially for small-data problems.\n",
    " \n",
    "5. **Developing a model that does better than a baseline**  \n",
    "    You need to make three key choices to build your first working model:\n",
    "    * Last-layer activation—This establishes useful constraints on the network’s out- put. For instance, the IMDB classification example used sigmoid in the last layer; the regression example didn’t use any last-layer activation; and so on.\n",
    "    * Loss function—This should match the type of problem you’re trying to solve. For instance, the IMDB example used binary_crossentropy, the regression exam- ple used mse, and so on.\n",
    "    * Optimization configuration—What optimizer will you use? What will its learning rate be? In most cases, it’s safe to go with rmsprop and its default learning rate.\n",
    "    \n",
    "          PROBLEM TYPE                             LAST-LAYER ACTIBATION      LOSS FUNCTION\n",
    "          \n",
    "          Binary classification                        sigmoid                binary_crossentropy\n",
    "          Multiclass, single-label classification      softmax                categorical_crossentropy\n",
    "          Multiclass, multilabel classification        sigmoid                Binary_crossentropy\n",
    "          Regression to arbitrary values               None                   mse\n",
    "          Regression to values between 0 and 1         sigmoid                mse or binary_crossentropy\n",
    "                       \n",
    "6. **Scaling up: developing a model that overfits**  \n",
    "   The ideal model is one that stands right at the border between underfitting and overfitting; between undercapacity and overcapacity. To fig- ure out where this border lies, first you must cross it.  \n",
    "   To figure out how big a model you’ll need, you must develop a model that overfits. This is fairly easy:\n",
    "   1. Add layers.\n",
    "   2. Make the layers bigger.\n",
    "   3. Train for more epochs.  \n",
    "   When you see that the model’s performance on the validation data begins to degrade, you’ve achieved overfitting.\n",
    "   \n",
    "7. **Regularizing your model and tuning your hyperparameters**\n",
    "   This step will take the most time: you’ll repeatedly modify your model, train it, evalu- ate on your validation data (not the test data, at this point), modify it again, and repeat, until the model is as good as it can get. These are some things you should try:\n",
    "   * Add dropout.\n",
    "   * Try different architectures: add or remove layers. \n",
    "   * Add L1 and/or L2 regularization.\n",
    "   * Try different hyperparameters (such as the number of units per layer or the learning rate of the optimizer) to find the optimal configuration.\n",
    "   * Optionally, iterate on feature engineering: add new features, or remove fea- tures that don’t seem to be informative.\n",
    "                         \n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "* Define the problem at hand and the data on which you’ll train. Collect this data, or annotate it with labels if need be.\n",
    "* Choose how you’ll measure success on your problem. Which metrics will you monitor on your validation data?\n",
    "* Determine your evaluation protocol: hold-out validation? K-fold valida- tion? Which portion of the data should you use for validation?\n",
    "* Develop a first model that does better than a basic baseline: a model with statistical power.\n",
    "* Develop a model that overfits.\n",
    "* Regularize your model and tune its hyperparameters, based on perfor- mance on the validation data. A lot of machine-learning research tends to focus only on this step—but keep the big picture in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
